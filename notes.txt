2019-12-19
At this point, I can use HPC to collect seed and candidate papers in parallel. One batch job will take a review article from WoS, find all the references, and generate five different seed/candidate splits (with five random seeds).
The previous analysis used MAG, and the results in the `jp_autoreview` MySQL database has MAG IDs for its review articles.
Next: collect all of the WoS IDs for the review articles we analyzed previously.
Used `collect_wos_review_ids.py` to collect 795 WoS IDs


2019-12-20
On hyak:
tried running job array with 795 jobs
Didn't work very well. Most timed out. Will try running separate jobs without using job array.
Getting some "disk quota exceeded" errors from spark


2019-12-27
create a sample of midsize (200-250 references) review articles. sample size of 100.
```
python create_midsize_sample.py ./data/wos_papers_reviews_with_citation_count.tsv ./data/reviews_wos_ids_midsize_sample_20191227.tsv --debug >& logs/create_midsize_sample_20191227.log &
```
