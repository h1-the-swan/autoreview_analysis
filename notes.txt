2019-12-19
At this point, I can use HPC to collect seed and candidate papers in parallel. One batch job will take a review article from WoS, find all the references, and generate five different seed/candidate splits (with five random seeds).
The previous analysis used MAG, and the results in the `jp_autoreview` MySQL database has MAG IDs for its review articles.
Next: collect all of the WoS IDs for the review articles we analyzed previously.
Used `collect_wos_review_ids.py` to collect 795 WoS IDs


2019-12-20
On hyak:
tried running job array with 795 jobs
Didn't work very well. Most timed out. Will try running separate jobs without using job array.
Getting some "disk quota exceeded" errors from spark
[jporteno@mox1 autoreview_analysis]$ mmlsquota -j stf gscratch
                         Block Limits                                    |     File Limits
			 Filesystem type             KB      quota      limit   in_doubt    grace |    files   quota    limit in_doubt    grace  Remarks
			 gscratch   FILESET  52591985408 48234496000 53057945600   20227200   7 days | 10901455 69000000 75900000    28868   

delete contents of spark_local_dir:
[jporteno@mox1 autoreview_analysis]$ rm -rf spark_local_dir/*
Takes a long time

Afterward:
[jporteno@mox1 autoreview_analysis]$ mmlsquota -j stf gscratch
                         Block Limits                                    |     File Limits
			 Filesystem type             KB      quota      limit   in_doubt    grace |    files   quota    limit in_doubt    grace  Remarks
			 gscratch   FILESET  41366808832 48234496000 53057945600   34544768     none |  9294846 69000000 75900000    17493     none 
